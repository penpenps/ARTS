本周分享一篇Google Could Newletter上关于AI行业发展的思考的文章:
[Steering the right course for AI](https://cloud.google.com/blog/products/ai-machine-learning/steering-the-right-course-for-ai)


文章开始提出，AI发展的这些年给人类带了很多好处，像帮助医生诊断，智能翻译，智能客服等。但同时也引起了人类对这项技术的担忧，就像上世纪末互联网兴起，带来了邮件，短信这样拉近人与人距离的技术，但同时无法避免恶意软件和钓鱼邮件的传播。近年来AI技术的发展比当时互联网的发展要复杂的多，也会有各种各样的问题出现，笔者提出了4个AI技术发展中必须考虑到的问题：
- **不公平性**: 我们如何保证AI训练出来的模型对于每个用户的表现是公平的？
    
    AI或者ML训练模型时使用的数据是由人来准备的，这就意味着模型学习什么样的数据，决定权在于人，如果这个人有自己的偏好，很有可能丢进模型训练的数据是片面的，模型也会有“刻板印象”。举个例子，如果我们给一个职业推荐模型的训练数据是一些旧时代的言论，它在推荐时很可能的偏好是认为，“男性”应该做“医生”，“女性”就应该做“护士”。如果我们给它一些过激歧视性言论，甚至会有种族问题。为了避免这个问题，Google现在采用文档化模型的设计，将模型要解决的问题，提出的假设，performance评判的指标，甚至伦理道德方面的考量等，创造出对用户负责的模型。

- **可解释性**: AI技术如何赢得人们的信任？

    目前来说，很多模型都是“黑盒”的，哪怕是模型开发人员知道输入和输出数据的含义，但并不是100%的肯定为什么模型要给出这样的结果。这样的不透明性，很难让用户完全信任模型给出的结果，信任是来自于理解这个模型的运作原理。Google AI在尝试让模型输出一些让人类更好理解的解释，而不是一些数字，量化后的指标。例如在图像识别中，将一张图片归类为“斑马”，是因为找到了“条状”的特征，理想情况下，模型应该用人类的语言向人类解释它为什么这么做。

- **工作模式的改变**: AI真的会代替人类的工作，让大部分人失业吗？

  AI并不是用机器完全替代人类，是将重复简单的工作自动化，让人可以更加专注于需要创造力，更有挑战的任务，帮助人类更好的工作。但不得不承认，有些工作是可以被替代的，为了帮助这部分人，Google成立了一个基金，帮助非盈利性的组织利用AI技术从三个方面入手：
  1. 提供继续教育和培训，帮助那些可能会失业的人可以找到新的工作方向
  2. 根据工作能力和背景，帮助人们找到更加合适的工作
  3. 为低收入的雇员提供帮助

- **做正确的事情**: 并不是所有的AI技术都是运用在正确的事情上，也有人拿来做“坏事”
  为了引导AI从业人员在设计模型时有更好的道德观念，Google发布了[AI at Google: our principles](https://www.blog.google/technology/ai/ai-principles/)一些在使用AI技术时应该考虑的准则。同时，也有Google AI的专家在给其他企业或个人提供AI方面咨询时，传播这些观念。

最后这个问题可能是最难用技术手段解决的，完全靠人的道德标准和互相的影响来改善。不过这样反应出值得欣慰的一点是，在思想观念层面，AI或许是无法替代人类的。
